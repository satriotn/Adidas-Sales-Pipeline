[2024-11-08T09:37:03.048+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [queued]>
[2024-11-08T09:37:03.068+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [queued]>
[2024-11-08T09:37:03.070+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-11-08T09:37:03.114+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): insert_to_db> on 2024-11-07 06:30:00+00:00
[2024-11-08T09:37:03.126+0000] {standard_task_runner.py:60} INFO - Started process 197 to run task
[2024-11-08T09:37:03.151+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'etl_csv_file', 'insert_to_db', 'scheduled__2024-11-07T06:30:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_csv_file.py', '--cfg-path', '/tmp/tmph09my1fh']
[2024-11-08T09:37:03.159+0000] {standard_task_runner.py:88} INFO - Job 3: Subtask insert_to_db
[2024-11-08T09:37:03.326+0000] {task_command.py:423} INFO - Running <TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [running]> on host fcf5fca827a9
[2024-11-08T09:37:03.481+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Satrio' AIRFLOW_CTX_DAG_ID='etl_csv_file' AIRFLOW_CTX_TASK_ID='insert_to_db' AIRFLOW_CTX_EXECUTION_DATE='2024-11-07T06:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-07T06:30:00+00:00'
[2024-11-08T09:37:04.792+0000] {logging_mixin.py:188} INFO - Data inserted into PostgreSQL
[2024-11-08T09:37:04.810+0000] {python.py:202} INFO - Done. Returned value was: None
[2024-11-08T09:37:04.843+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=etl_csv_file, task_id=insert_to_db, execution_date=20241107T063000, start_date=20241108T093703, end_date=20241108T093704
[2024-11-08T09:37:04.938+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-11-08T09:37:04.997+0000] {taskinstance.py:3312} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-11-08T09:43:55.711+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [queued]>
[2024-11-08T09:43:55.736+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [queued]>
[2024-11-08T09:43:55.738+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-11-08T09:43:55.769+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): insert_to_db> on 2024-11-07 06:30:00+00:00
[2024-11-08T09:43:55.776+0000] {standard_task_runner.py:60} INFO - Started process 196 to run task
[2024-11-08T09:43:55.789+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'etl_csv_file', 'insert_to_db', 'scheduled__2024-11-07T06:30:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/etl_csv_file.py', '--cfg-path', '/tmp/tmpztrih4gc']
[2024-11-08T09:43:55.801+0000] {standard_task_runner.py:88} INFO - Job 2: Subtask insert_to_db
[2024-11-08T09:43:55.971+0000] {task_command.py:423} INFO - Running <TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [running]> on host d2f121bfe37c
[2024-11-08T09:43:56.128+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Satrio' AIRFLOW_CTX_DAG_ID='etl_csv_file' AIRFLOW_CTX_TASK_ID='insert_to_db' AIRFLOW_CTX_EXECUTION_DATE='2024-11-07T06:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-07T06:30:00+00:00'
[2024-11-08T09:43:57.431+0000] {logging_mixin.py:188} INFO - Data inserted into PostgreSQL
[2024-11-08T09:43:57.434+0000] {python.py:202} INFO - Done. Returned value was: None
[2024-11-08T09:43:57.449+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=etl_csv_file, task_id=insert_to_db, execution_date=20241107T063000, start_date=20241108T094355, end_date=20241108T094357
[2024-11-08T09:43:57.519+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-11-08T09:43:57.616+0000] {taskinstance.py:3312} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-11-08T09:56:21.817+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [queued]>
[2024-11-08T09:56:21.837+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [queued]>
[2024-11-08T09:56:21.838+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-11-08T09:56:21.863+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): insert_to_db> on 2024-11-07 06:30:00+00:00
[2024-11-08T09:56:21.868+0000] {standard_task_runner.py:60} INFO - Started process 197 to run task
[2024-11-08T09:56:21.874+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'etl_csv_file', 'insert_to_db', 'scheduled__2024-11-07T06:30:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/etl_csv_file.py', '--cfg-path', '/tmp/tmpma_pgg57']
[2024-11-08T09:56:21.878+0000] {standard_task_runner.py:88} INFO - Job 2: Subtask insert_to_db
[2024-11-08T09:56:21.990+0000] {task_command.py:423} INFO - Running <TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [running]> on host f7a8b7587482
[2024-11-08T09:56:22.148+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Satrio' AIRFLOW_CTX_DAG_ID='etl_csv_file' AIRFLOW_CTX_TASK_ID='insert_to_db' AIRFLOW_CTX_EXECUTION_DATE='2024-11-07T06:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-07T06:30:00+00:00'
[2024-11-08T09:56:23.524+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "pg_type_typname_nsp_index"
DETAIL:  Key (typname, typnamespace)=(adidas_sales_raw, 2200) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/etl_csv_file.py", line 42, in insert_to_db
    df.to_sql('adidas_sales_raw', conn, index=False, if_exists='replace')
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 769, in to_sql
    return pandas_sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1910, in to_sql
    table = self.prep_table(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1814, in prep_table
    table.create()
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 914, in create
    self._execute_create()
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 900, in _execute_create
    self.table.create(bind=self.pd_sql.con)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/schema.py", line 962, in create
    bind._run_ddl_visitor(ddl.SchemaGenerator, self, checkfirst=checkfirst)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2221, in _run_ddl_visitor
    visitorcallable(self.dialect, self, **kwargs).traverse_single(element)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/visitors.py", line 524, in traverse_single
    return meth(obj, **kw)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/ddl.py", line 899, in visit_table
    self.connection.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1385, in execute
    return meth(self, multiparams, params, _EMPTY_EXECUTION_OPTS)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/ddl.py", line 80, in _execute_on_connection
    return connection._execute_ddl(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1477, in _execute_ddl
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "pg_type_typname_nsp_index"
DETAIL:  Key (typname, typnamespace)=(adidas_sales_raw, 2200) already exists.

[SQL: 
CREATE TABLE adidas_sales_raw (
	"Unnamed: 0" BIGINT, 
	"Invoice Id" BIGINT, 
	"Retailer" TEXT, 
	"Retailer ID" BIGINT, 
	"Invoice Date" TEXT, 
	"Region" TEXT, 
	"State" TEXT, 
	"City" TEXT, 
	"Product" TEXT, 
	"Price per Unit" FLOAT(53), 
	"Units Sold" FLOAT(53), 
	"Total Sales" BIGINT, 
	"Operating Profit" BIGINT, 
	"Operating Margin" BIGINT, 
	"Sales Method" TEXT
)

]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-11-08T09:56:23.579+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=etl_csv_file, task_id=insert_to_db, execution_date=20241107T063000, start_date=20241108T095621, end_date=20241108T095623
[2024-11-08T09:56:23.608+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 2 for task insert_to_db ((psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "pg_type_typname_nsp_index"
DETAIL:  Key (typname, typnamespace)=(adidas_sales_raw, 2200) already exists.

[SQL: 
CREATE TABLE adidas_sales_raw (
	"Unnamed: 0" BIGINT, 
	"Invoice Id" BIGINT, 
	"Retailer" TEXT, 
	"Retailer ID" BIGINT, 
	"Invoice Date" TEXT, 
	"Region" TEXT, 
	"State" TEXT, 
	"City" TEXT, 
	"Product" TEXT, 
	"Price per Unit" FLOAT(53), 
	"Units Sold" FLOAT(53), 
	"Total Sales" BIGINT, 
	"Operating Profit" BIGINT, 
	"Operating Margin" BIGINT, 
	"Sales Method" TEXT
)

]
(Background on this error at: https://sqlalche.me/e/14/gkpj); 197)
[2024-11-08T09:56:23.662+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-11-08T09:56:23.719+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-11-08T10:11:01.713+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [queued]>
[2024-11-08T10:11:01.733+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [queued]>
[2024-11-08T10:11:01.734+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-11-08T10:11:01.770+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): insert_to_db> on 2024-11-07 06:30:00+00:00
[2024-11-08T10:11:01.790+0000] {standard_task_runner.py:60} INFO - Started process 191 to run task
[2024-11-08T10:11:01.799+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'etl_csv_file', 'insert_to_db', 'scheduled__2024-11-07T06:30:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/etl_csv_file.py', '--cfg-path', '/tmp/tmpach_j0co']
[2024-11-08T10:11:01.806+0000] {standard_task_runner.py:88} INFO - Job 4: Subtask insert_to_db
[2024-11-08T10:11:01.949+0000] {task_command.py:423} INFO - Running <TaskInstance: etl_csv_file.insert_to_db scheduled__2024-11-07T06:30:00+00:00 [running]> on host efe6839ec606
[2024-11-08T10:11:02.156+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Satrio' AIRFLOW_CTX_DAG_ID='etl_csv_file' AIRFLOW_CTX_TASK_ID='insert_to_db' AIRFLOW_CTX_EXECUTION_DATE='2024-11-07T06:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-07T06:30:00+00:00'
[2024-11-08T10:11:05.214+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "pg_type_typname_nsp_index"
DETAIL:  Key (typname, typnamespace)=(adidas_sales_raw, 2200) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/etl_csv_file.py", line 42, in insert_to_db
    df.to_sql('adidas_sales_raw', conn, index=False, if_exists='replace')
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 769, in to_sql
    return pandas_sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1910, in to_sql
    table = self.prep_table(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1814, in prep_table
    table.create()
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 914, in create
    self._execute_create()
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 900, in _execute_create
    self.table.create(bind=self.pd_sql.con)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/schema.py", line 962, in create
    bind._run_ddl_visitor(ddl.SchemaGenerator, self, checkfirst=checkfirst)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2221, in _run_ddl_visitor
    visitorcallable(self.dialect, self, **kwargs).traverse_single(element)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/visitors.py", line 524, in traverse_single
    return meth(obj, **kw)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/ddl.py", line 899, in visit_table
    self.connection.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1385, in execute
    return meth(self, multiparams, params, _EMPTY_EXECUTION_OPTS)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/ddl.py", line 80, in _execute_on_connection
    return connection._execute_ddl(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1477, in _execute_ddl
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "pg_type_typname_nsp_index"
DETAIL:  Key (typname, typnamespace)=(adidas_sales_raw, 2200) already exists.

[SQL: 
CREATE TABLE adidas_sales_raw (
	"Unnamed: 0" BIGINT, 
	"Invoice Id" BIGINT, 
	"Retailer" TEXT, 
	"Retailer ID" BIGINT, 
	"Invoice Date" TEXT, 
	"Region" TEXT, 
	"State" TEXT, 
	"City" TEXT, 
	"Product" TEXT, 
	"Price per Unit" FLOAT(53), 
	"Units Sold" FLOAT(53), 
	"Total Sales" BIGINT, 
	"Operating Profit" BIGINT, 
	"Operating Margin" BIGINT, 
	"Sales Method" TEXT
)

]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-11-08T10:11:05.616+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=etl_csv_file, task_id=insert_to_db, execution_date=20241107T063000, start_date=20241108T101101, end_date=20241108T101105
[2024-11-08T10:11:05.786+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 4 for task insert_to_db ((psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "pg_type_typname_nsp_index"
DETAIL:  Key (typname, typnamespace)=(adidas_sales_raw, 2200) already exists.

[SQL: 
CREATE TABLE adidas_sales_raw (
	"Unnamed: 0" BIGINT, 
	"Invoice Id" BIGINT, 
	"Retailer" TEXT, 
	"Retailer ID" BIGINT, 
	"Invoice Date" TEXT, 
	"Region" TEXT, 
	"State" TEXT, 
	"City" TEXT, 
	"Product" TEXT, 
	"Price per Unit" FLOAT(53), 
	"Units Sold" FLOAT(53), 
	"Total Sales" BIGINT, 
	"Operating Profit" BIGINT, 
	"Operating Margin" BIGINT, 
	"Sales Method" TEXT
)

]
(Background on this error at: https://sqlalche.me/e/14/gkpj); 191)
[2024-11-08T10:11:05.848+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-11-08T10:11:06.032+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
